{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1419ea22-d0cb-4e66-bdb0-74a55eb91028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac1024d-146a-48ba-b975-2e9790305ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implements: softmax( (QK^T) / sqrt(d_k) ) V\n",
    "    \n",
    "    Q, K, V shape: (batch, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 1. Raw attention scores (QK^T)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (B, seq, seq)\n",
    "\n",
    "    # Softmax BEFORE scaling (stability check)\n",
    "    softmax_before = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 2. Scale by sqrt(d_k)\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Softmax AFTER scaling\n",
    "    softmax_after = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "    # 3. Final attention weights\n",
    "    attn_weights = softmax_after\n",
    "\n",
    "    # 4. Attention output = weights * V\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "\n",
    "    return attn_weights, output, softmax_before, softmax_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3b23e4-75b3-45ee-ac3e-767bfdec3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "batch = 1\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "\n",
    "# Random Q, K, V\n",
    "Q = torch.randn(batch, seq_len, d_k)\n",
    "K = torch.randn(batch, seq_len, d_k)\n",
    "V = torch.randn(batch, seq_len, d_k)\n",
    "\n",
    "# Run attention\n",
    "attn_w, out, soft_before, soft_after = scaled_dot_product_attention(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5b44ba-bb34-4b4f-b732-30296b421580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Attention Weight Matrix ===\n",
      "tensor([[[0.4929, 0.1561, 0.2338, 0.1171],\n",
      "         [0.0238, 0.2865, 0.5080, 0.1818],\n",
      "         [0.2184, 0.2550, 0.2867, 0.2399],\n",
      "         [0.0476, 0.0218, 0.0434, 0.8872]]])\n",
      "\n",
      "=== Output Vectors ===\n",
      "tensor([[[ 0.2389,  0.1452,  0.3632,  0.4489,  0.0188,  0.8193,  0.7235,\n",
      "           0.3997],\n",
      "         [-0.2456, -0.0512, -0.1037,  0.6577, -0.2569,  0.3588,  0.9007,\n",
      "           0.3187],\n",
      "         [ 0.2671,  0.0984,  0.2333,  0.5416, -0.0735,  0.5456,  0.7390,\n",
      "           0.1525],\n",
      "         [ 1.5190,  0.3850,  0.7770,  0.8229, -0.4060,  0.1993,  0.3198,\n",
      "          -2.0086]]])\n",
      "\n",
      "=== Softmax BEFORE scaling ===\n",
      "tensor([[[8.4944e-01, 3.2886e-02, 1.0308e-01, 1.4594e-02],\n",
      "         [1.3814e-04, 1.5796e-01, 7.9829e-01, 4.3613e-02],\n",
      "         [1.6636e-01, 2.5772e-01, 3.5900e-01, 2.1692e-01],\n",
      "         [2.5505e-04, 2.7991e-05, 1.9590e-04, 9.9952e-01]]])\n",
      "\n",
      "=== Softmax AFTER scaling ===\n",
      "tensor([[[0.4929, 0.1561, 0.2338, 0.1171],\n",
      "         [0.0238, 0.2865, 0.5080, 0.1818],\n",
      "         [0.2184, 0.2550, 0.2867, 0.2399],\n",
      "         [0.0476, 0.0218, 0.0434, 0.8872]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Attention Weight Matrix ===\")\n",
    "print(attn_w)\n",
    "\n",
    "print(\"\\n=== Output Vectors ===\")\n",
    "print(out)\n",
    "\n",
    "print(\"\\n=== Softmax BEFORE scaling ===\")\n",
    "print(soft_before)\n",
    "\n",
    "print(\"\\n=== Softmax AFTER scaling ===\")\n",
    "print(soft_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ede83a-1b56-4bf7-a4ce-e84ec71ab522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
